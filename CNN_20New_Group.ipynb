{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>review</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['From: strom@Watson.Ibm.Com (Rob Strom)\\n', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phrase_id                                             review  group_id\n",
       "0          1  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "1          2  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "2          3  ['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...         0\n",
       "3          4  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "4          5  ['From: strom@Watson.Ibm.Com (Rob Strom)\\n', '...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pandas import Series, DataFrame\n",
    "group_train = pd.read_csv(\"C:\\\\Users\\\\karis\\\\NewGroup_data.txt\", sep=\"\\t\", header = None, names= ['phrase_id', 'review', 'group_id'])\n",
    "group_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>review</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['From: decay@cbnewsj.cb.att.com (dean.kaflowi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['From: cfaehl@vesta.unm.edu (Chris Faehl)\\n',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['From: dps@nasa.kodak.com (Dan Schaertel,,,)\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['From: halat@panther.bears (Jim Halat)\\n', 'S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phrase_id                                             review  group_id\n",
       "0          1  ['From: decay@cbnewsj.cb.att.com (dean.kaflowi...         0\n",
       "1          2  ['From: cfaehl@vesta.unm.edu (Chris Faehl)\\n',...         0\n",
       "2          3  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "3          4  ['From: dps@nasa.kodak.com (Dan Schaertel,,,)\\...         0\n",
       "4          5  ['From: halat@panther.bears (Jim Halat)\\n', 'S...         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_test = pd.read_csv(\"C:\\\\Users\\\\karis\\\\NewGroup_data_test.txt\", sep=\"\\t\", header = None, names= ['phrase_id', 'review', 'group_id'])\n",
    "group_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maping cateogories to train dataframe\n",
    "\n",
    "group_train['category'] = group_train.group_id.map({0:'alt', 1:'comp',2:'comp', 3:'comp', 4:'comp',5:'comp',6:'misc', 7:'rec', 8:'rec',9:'rec', 10:'rec', 11:'sci',\n",
    "                                    12:'sci', 13:'sci', 14:'sci', 15:'religion', \n",
    "                                    16:'politics', 17:'politics', 18:'politics', 19:'religion'})\n",
    "\n",
    "# maping cateogories to test dataframe\n",
    "\n",
    "group_test['category'] = group_test.group_id.map({0:'alt', 1:'comp',2:'comp', 3:'comp', 4:'comp',5:'comp',6:'misc', 7:'rec', 8:'rec',9:'rec', 10:'rec', 11:'sci',\n",
    "                                    12:'sci', 13:'sci', 14:'sci', 15:'religion', \n",
    "                                    16:'politics', 17:'politics', 18:'politics', 19:'religion'})\n",
    "\n",
    "group_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# selecting categories \n",
    "group_train_category = group_train[(group_train.category == 'comp') | (group_train.category == 'rec') | (group_train.category == 'politics') | (group_train.category == 'religion')]\n",
    "group_test_category = group_test[(group_test.category == 'comp') | (group_test.category == 'rec') | (group_test.category == 'politics') | (group_test.category == 'religion')]\n",
    "\n",
    "\n",
    "\n",
    "# # spliting data into input X_train and output Y_train\n",
    "# X_train = group_train_category.review\n",
    "# # Y_train = group_train_category.category\n",
    "\n",
    "# # spliting data into input X_train and output Y_train\n",
    "# X_test = group_test_category.review\n",
    "# # Y_test = group_test_category.category\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# group_train_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karis\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\karis\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "group_train_category['category_num'] = group_train_category.category.map({'comp':0, 'rec':1, 'politics':2, 'religion':3})\n",
    "group_test_category['category_num'] = group_test_category.category.map({'comp':0, 'rec':1, 'politics':2, 'religion':3})\n",
    "\n",
    "group_train_test_category = pd.concat([group_train_category, group_test_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7876, 5)\n",
      "(5244, 5)\n",
      "(13120, 5)\n"
     ]
    }
   ],
   "source": [
    "print(group_train_category.shape)\n",
    "print(group_test_category.shape)\n",
    "print(group_train_test_category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13120, 4)\n",
      "(13120,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "Y = np_utils.to_categorical(group_train_test_category['category_num'])\n",
    "X = group_train_test_category.review\n",
    "\n",
    "# Y_test_label_encoded = np_utils.to_categorical(np.asarray(group_test_category['category']))\n",
    "\n",
    "\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Label_encoded = LabelEncoder()\n",
    "# Y_train_label_encoded = Label_encoded.fit_transform(group_train_category['category'].values)\n",
    "# Y_test_label_encoded = Label_encoded.fit_transform(group_test_category['category'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 172367 unique tokens.\n",
      "Shape of data tensor: (13120, 1000)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "X_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(X_word_index))\n",
    "\n",
    "X_data = pad_sequences(X_sequences, maxlen=1000)\n",
    "\n",
    "print('Shape of data tensor:', X_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "f = os.path.join('C:/Users/karis/Desktop/latently', 'glove.6B.100d.txt')\n",
    "with open(f,encoding='utf-8') as main:\n",
    "    for line in main:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(X_word_index) + 1, 100))\n",
    "for word, i in X_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172368, 100)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(X_word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=1000,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n",
      "(10000, 1000)\n",
      "(3120, 4)\n",
      "(3120, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_data[:10000]  \n",
    "X_test = X_data[10000:] \n",
    "Y_train = Y[:10000]\n",
    "Y_test = Y[10000:]\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-eead0413fb81>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-eead0413fb81>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    model.add(Dense(250, activation= 'relu' ))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "max_words = 500\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(X_word_index) + 1, 100, weights=[embedding_matrix], input_length=1000, trainable=False)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu' ))\n",
    "model.add(Dense(1, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128,\n",
    "verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sequence_input = Input(shape=(1000,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "# x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(5)(x)\n",
    "# x = Conv1D(128, 5, activation='relu')(x)\n",
    "# x = MaxPooling1D(35)(x)  # global max pooling\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# preds = Dense(len(Y_train), activation='softmax')(x)\n",
    "\n",
    "# model = Model(sequence_input, preds)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "\n",
    "# # happy learning!\n",
    "# model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=2, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
