{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>review</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['From: strom@Watson.Ibm.Com (Rob Strom)\\n', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phrase_id                                             review  group_id\n",
       "0          1  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "1          2  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "2          3  ['From: I3150101@dbstu1.rz.tu-bs.de (Benedikt ...         0\n",
       "3          4  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "4          5  ['From: strom@Watson.Ibm.Com (Rob Strom)\\n', '...         0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pandas import Series, DataFrame\n",
    "group_train = pd.read_csv(\"C:\\\\Users\\\\karis\\\\NewGroup_data.txt\", sep=\"\\t\", header = None, names= ['phrase_id', 'review', 'group_id'])\n",
    "group_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_id</th>\n",
       "      <th>review</th>\n",
       "      <th>group_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['From: decay@cbnewsj.cb.att.com (dean.kaflowi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['From: cfaehl@vesta.unm.edu (Chris Faehl)\\n',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>['From: mathew &lt;mathew@mantis.co.uk&gt;\\n', 'Subj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['From: dps@nasa.kodak.com (Dan Schaertel,,,)\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['From: halat@panther.bears (Jim Halat)\\n', 'S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phrase_id                                             review  group_id\n",
       "0          1  ['From: decay@cbnewsj.cb.att.com (dean.kaflowi...         0\n",
       "1          2  ['From: cfaehl@vesta.unm.edu (Chris Faehl)\\n',...         0\n",
       "2          3  ['From: mathew <mathew@mantis.co.uk>\\n', 'Subj...         0\n",
       "3          4  ['From: dps@nasa.kodak.com (Dan Schaertel,,,)\\...         0\n",
       "4          5  ['From: halat@panther.bears (Jim Halat)\\n', 'S...         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_test = pd.read_csv(\"C:\\\\Users\\\\karis\\\\NewGroup_data_test.txt\", sep=\"\\t\", header = None, names= ['phrase_id', 'review', 'group_id'])\n",
    "group_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maping cateogories to train dataframe\n",
    "\n",
    "group_train['category'] = group_train.group_id.map({0:'alt', 1:'comp',2:'comp', 3:'comp', 4:'comp',5:'comp',6:'misc', 7:'rec', 8:'rec',9:'rec', 10:'rec', 11:'sci',\n",
    "                                    12:'sci', 13:'sci', 14:'sci', 15:'religion', \n",
    "                                    16:'politics', 17:'politics', 18:'politics', 19:'religion'})\n",
    "\n",
    "# maping cateogories to test dataframe\n",
    "\n",
    "group_test['category'] = group_test.group_id.map({0:'alt', 1:'comp',2:'comp', 3:'comp', 4:'comp',5:'comp',6:'misc', 7:'rec', 8:'rec',9:'rec', 10:'rec', 11:'sci',\n",
    "                                    12:'sci', 13:'sci', 14:'sci', 15:'religion', \n",
    "                                    16:'politics', 17:'politics', 18:'politics', 19:'religion'})\n",
    "\n",
    "group_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selecting categories \n",
    "group_train_category = group_train[(group_train.category == 'comp') | (group_train.category == 'rec') | (group_train.category == 'politics') | (group_train.category == 'religion')]\n",
    "group_test_category = group_test[(group_test.category == 'comp') | (group_test.category == 'rec') | (group_test.category == 'politics') | (group_test.category == 'religion')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatinating Train and Test datasets\n",
    "\n",
    "group_train_test_category = pd.concat([group_train_category, group_test_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13120, 4)\n",
      "(13120,)\n",
      "[0 0 0 ..., 3 3 3]\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "Label_encoded = LabelEncoder()\n",
    "Y_label_encoded = Label_encoded.fit_transform(group_train_test_category['category'].values)\n",
    "Y = np_utils.to_categorical(Y_label_encoded)\n",
    "X = group_train_test_category.review\n",
    "\n",
    "\n",
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "print(Y_label_encoded)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 172367 unique tokens.\n",
      "Shape of data tensor: (13120, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "X_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(X_word_index))\n",
    "\n",
    "X_data = pad_sequences(X_sequences, maxlen=500)\n",
    "\n",
    "print('Shape of data tensor:', X_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train =X_data[:10000]\n",
    "X_test = X_data[10000:]\n",
    "Y_train = Y[:10000]\n",
    "Y_test =Y[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/karis/Desktop/latently/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.syn0\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import losses\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-4bcb2d88e5c5>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-4bcb2d88e5c5>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model.add(Conv1D(nb_filter=32, filter_length=3, border_mode= 'same' , activation= 'relu', input_length=10000))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=weights.shape[0], output_dim=weights.shape[1], weights=[weights])\n",
    "model.add(Conv1D(nb_filter=32, filter_length=3, border_mode= 'same' , activation= 'relu', input_length=10000))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu' ))\n",
    "model.add(Dense(4, activation= 'softmax' ))\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), nb_epoch=10, batch_size=128, verbose=1)\n",
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentences = []\n",
    "# for review in group_train_test_category.review:\n",
    "#     review = review.split()\n",
    "#     sentences.append(review)\n",
    "\n",
    "# model['sentences'].shape\n",
    "# word2vec = Word2Vec(sentences, workers=2)\n",
    "\n",
    "#  model.wv.vocab.items()\n",
    "\n",
    "# # model.wv.syn0\n",
    "\n",
    "\n",
    "# embeddings = np.zeros((word2vec.wv.syn0.shape[0] + 1, word2vec.wv.syn0.shape[1]), dtype = \"float32\")\n",
    "\n",
    "\n",
    "\n",
    "# embeddings[:model.wv.syn0.shape[0]] = word2vec.wv.syn0\n",
    "# # embeddings.shape\n",
    "# word2vec.wv.syn0.shape\n",
    "\n",
    "# vocab = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "\n",
    "# model.most_similar('good')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
